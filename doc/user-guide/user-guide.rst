
.. Fill out data so contacts section below is auto-populated
.. add name and email between the *'s below e.g. *Marie Smith <msmith@lsst.org>*
.. |CSC_developer| replace::  *Tiago Ribeiro*
.. |CSC_product_owner| replace:: *Tiago Ribeiro*

.. _User_Guide:

#######################
Build Cycle User Guide
#######################

.. Update links and labels below
.. image:: https://img.shields.io/badge/GitHub-ts_cycle_build-green.svg
    :target: https://github.com/lsst-ts/ts_cycle_build
.. image:: https://img.shields.io/badge/Jenkins-ts_cycle_build-green.svg
    :target: https://tssw-ci.lsst.org/view/CSC_Docker/job/cycleBuild/
.. image:: https://img.shields.io/badge/Jira-ts_cycle_build.svg
    :target: https://jira.lsstcorp.org/issues/?jql=labels+%3D+ts_cycle_build

The highly coupled nature of the Vera Rubin observatory control system demands a tight control over deployment components.
There are two important factors that contribute to this nature of the system, the component's interface and the communication middleware.

The component's interface is similar to a database schema.
It defines the input/output data structure for each individual component, in addition to a set of generic input/output imposed by the system architecture.
Even though components are designed as independent actors they are not isolated entities, and must communicate with each other to accomplish higher level operations.
For example, the Main Telescope Mount component (``MTMount``) must be able to track targets in response to data generated by the Main Telescope Pointing component (``MTPtg``).
In order for this to work properly ``MTPtg`` must have the correct version of the interface of ``MTMount`` or they would not be able to co-operate.

This issue is intensified by the architecture of communication middleware, Data Distribution Service (DDS).
DDS relies on strongly typed Topics, e.g. data structures similar to C structs, to exchange data between hosts.
For hosts to be able to communicate they must know the Topic type beforehand.
If the version of the Topics on two hosts is different, whoever tries to subscribe last (be it to read or write) will fail.
In general this problem can be avoided by use of loosely typed topics and schema evolution, which allows readers and writers to exchange any type of data structure and translate different versions.

Furthermore, DDS also contains a highly diverse Quality of Service (QoS) configuration that is applied on an individual topic level.
In order for the communication to work properly readers and writers must also agree on the QoS settings.
On latest versions of SAL and SalObj (the software in charge of managing the DDS communication) the settings are fully specified in a configuration file that is defined in the ``ts_idl`` package.

Altogether, these properties of the system mean we must maintain tight control and management over deployment artifacts.
In order to facilitate management of the build and deployment structure in these conditions we introduced the concept of cycle build.
Basically the cycle build is subdivided in two levels; cycle and revision.

Anytime a version of a core package changes a new cycle must be created.
If the version of an individual package or dependency library changed, outside of the core packages, a new revision is created.
The build is then tagged accordingly;

.. prompt:: bash

  c<cycle-number>.<revision>

All the images are tagged accordingly.

For instance, the image for the ``MTMount`` component for cycle 14, revision 3, will receive the name:

.. prompt:: bash

  ts-dockerhub.lsst.org/mtmount:c0014.003

The first part of the name is the nexus docker registry where we store the deployable artifacts, the second is the name of the image followed by the tag.

In addition, for each cycle the latest revision is tagged with the cycle only, e.g.;

.. prompt:: bash

  ts-dockerhub.lsst.org/mtmount:c0014

This allows for easily switching back and forth between revisions of a single cycle.
At the same time, all images are annotated with the version information about the installed packages so one can easily inspect the content by doing:

.. prompt:: bash

  docker image inspect -f {{.Config.Labels}} ts-dockerhub.lsst.org/mtmount:c<cycle-number>.<revision>

.. _Core-packages:

Core packages
-------------

  - ts_xml: Contains the definition of the interfaces of all components in the system.
  - ts_sal: Contains the core DDS communication middleware layer.
  - ts_idl: Contains ancillary files used by ts_salobj and other higher level applications.
  - ts_salobj: High-level Python library to develop CSCs and SAL Script.


.. _Building-a-new-Cycle-or-Revision:

Building a new Cycle or Revision
================================

There are two ways users can run a build; manually on a private server or using the `TSSW Jenkins server`_.
Since the build requires credentials for the nexus servers, both to access the licensed version of OpenSplice and to pull/push images to the nexus docker registry, it is advisable to use the `TSSW Jenkins server`_.

.. _TSSW Jenkins server: https://tssw-ci.lsst.org

.. note::

  At this point, cycle and revision are still managed manually by users.
  We are investigating ways of automating the process so users don't have to worry about setting them up.

The process to create a new cycle/revision build is to clone the repository and create a ticket branch to work on.

.. prompt:: bash

  git clone https://github.com/lsst-ts/ts_cycle_build.git
  cd ts_cycle_build/
  git checkout tickets/DM-XXXXX

Then, update the cycle/revision and software versions as needed.
If any of the :ref:`core packages <Core-packages>` version is updated, the cycle number must be incremented, otherwise just increment the revision number and update the software versions.

Both cycle, revision and all software versions are managed by an environment file located in:

.. prompt:: bash

  cycle/cycle.env

At the very top you will find the cycle and revision numbers.

Once this file is updated, you should be ready to start a build.
For that, commit and push your changes to GitHub.
Make sure to describe the changes you have made on the commit message and also update the version history.

Once the changes are pushed to GitHub the branch will appear in the `cycle build jenkins job`_.
Select the branch you are working on in the Branch tab and select Build with Parameters on the left-hand side, which will that you to the :ref:`fig-jenkins-build-with-parameters`.

.. _cycle build jenkins job: https://tssw-ci.lsst.org/view/CSC_Docker/job/cycleBuild/

.. figure:: /_static/JenkinsBuildWithParameters.png
   :name: fig-jenkins-build-with-parameters
   :target: ../_images/SingleProcessFig.png
   :alt: Jenkins build with parameters

   Jenkins build with parameters page.

The build is divided into different steps.
These steps are designed to maximized reusability of docker layers, minimizing the number of layers in the image and reducing the time it takes to build the system.
The steps in the build are as follows:

  - deploy_conda: Build base image used by all conda-installable components.
  - deploy_lsstsqre: Build base image used by components that require the DM stack.
  - base_components: Build all components that contains conda packages. This includes the following components.
    - ataos
    - atdome
    - atdometrajectory
    - athexapod
    - atmcs_sim
    - atpneumatics_sim
    - atspectrograph
    - hexapod
    - mtdome
    - mtdometrajectory
    - mtm2_sim
    - mtmount
    - ospl-daemon
    - rotator
    - salkafka
    - watcher
  - m1m3_sim: Build M1M3 simulator.
  - ptg: Build pointing component. Both AT and MT use the same code base and image.
  - lsst_sims: Build base image with ``lsst_sims``. Use ``deploy_lsstsqre`` as a base image.
  - aos_aoclc: Build base image for MTAOS. This adds a couple of dependencies used by MTAOS.
  - mtaos: Build MTAOS.
  - queue: Build ScriptQueue. Both AT and MT use the same code base and image.
  - scheduler: Build Scheduler. Both AT and MT use the same code base and image.
